{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script can be used as a standalone program to process and prepare topic modelling of the ```Scraper.py``` datasets generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 <u>Code:</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import the libraries required to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sarthak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Natural Language toolkit. Here we download the commonly used English stopwords'''\n",
    "import nltk; nltk.download('stopwords')\n",
    "'''Standard set of functions for reading and appending files'''\n",
    "import re\n",
    "'''Pandas and numpy is a dependency used by other portions of the code.'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "'''Think this stands for pretty print. Prints out stuff to the terminal in a prettier way'''\n",
    "from pprint import pprint\n",
    "\n",
    "'''Contains the language model that has to be developed.'''\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "'''Industrial level toolkit for NLP'''\n",
    "import spacy\n",
    "\n",
    "'''Importing OS here to build the directories to hold the status logger logs'''\n",
    "import os\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "'''Importing datetime here to log the time when specfic functions are triggered'''\n",
    "from datetime import datetime\n",
    "\n",
    "'''Make pretty visualizations'''\n",
    "import matplotlib as plt\n",
    "\n",
    "'''Library to log any errors. Came across this in the tutorial.'''\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'com', 'https', 'url', 'link', 'xe', 'abstract', 'author', 'chapter', 'springer', 'title', \"the\", \"of\", \"and\", \"in\", \"to\", \"a\", \"is\", \"for\", \"from\", \"with\", \"that\",\"by\", \"are\", \"on\", \"was\", \"as\", \"were\", \"url:\", \"abstract:\", \"abstract\",  \"author:\", \"title:\", \"at\", \"be\", \"an\", \"during\", \"have\", \"this\", \"which\", \"study\", \"been\", \"species\", \"not\", \"has\", \"between\", \"using\", \"its\", \"also\", \"these\", \"this\", \"used\", \"over\", \"can\", \"within\", \"into\", \"all\",\"due\", \"use\", \"about\", \"a\", 'it', 'their', \"where\", \"we\", \"most\", \"may\", \"through\", \"though\", \"like\", \"or\", \"further\", \"e.g.\", \"along\", \"any\", \"those\", \"had\", \"toward\", \"due\", \"both\", \"some\", \"use\", \"even\", \"more\", \"but\", \"while\", \"pass\", \"well\", \"will\", \"when\", \"only\", \"after\", \"author\", \"title\", \"there\", \"our\", \"did\", \"much\", \"as\", \"if\", \"become\", \"still\", \"various\", \"very\", \"out\", \"they\", \"via\", \"available\", \"such\", \"than\", \"different\", \"many\", \"areas\", \"no\", \"one\", \"two\", \"small\", \"first\", \"other\", \"such\", \"-\", \"could\", \"studies\", \"high\",\"provide\", \"among\", \"highly\", \"no\", \"case\", \"across\", \"given\", \"need\", \"would\", \"under\", \"found\", \"low\", \"values\", \"xe2\\\\x80\\\\x89\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the abstracts log name that will be processed by the rest of the script. The status logger function of the main script has been imported here, and the name of the ```.txt``` file used to log the information has to be added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_log_name = \"/home/sarthak/projects/Bias/BackUp_Journal_LOGS/Corpus_Bio_Hotspot_Data/JournalCorpusBiodiversity\"\n",
    "session_folder_name = abstracts_log_name.split('/')[7]\n",
    "os.makedirs(session_folder_name)\n",
    "status_logger_name = session_folder_name+\"/\"+\"Status_Logger\"+\"_\"+abstracts_log_name.split('/')[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is responsible for generating a ```.txt``` file for analysis once the code has run. This code has been copied directly from the ```common_functions.py``` script of the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_logger(status_logger_name, status_key):\n",
    "\t'''Status logger to print and log details throught the running the program.\n",
    "\tDeclaring current_hour, current_minute & current_second.'''\n",
    "\tcurrent_hour = str(datetime.now().time().hour)\n",
    "\tcurrent_minute = str(datetime.now().time().minute)\n",
    "\tcurrent_second = str(datetime.now().time().second)\n",
    "\t'''Logging the complete_status_key and printing the complete_status_key'''\n",
    "\tcomplete_status_key = \"[INFO]\"+current_hour+\":\"+current_minute+\":\"+current_second+\" \"+status_key\n",
    "\tprint(complete_status_key)\n",
    "\tstatus_log = open(status_logger_name+'.txt', 'a')\n",
    "\tstatus_log.write(complete_status_key+\"\\n\")\n",
    "\tstatus_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we read the ```.txt``` file generated from the journal scrapping, that has to be profiled using topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(abstracts_log_name, status_logger_name):\n",
    "\tdata_reader_start_status_key = abstracts_log_name+\".txt is being ported to dataframe\"\n",
    "\tstatus_logger(status_logger_name, data_reader_start_status_key)\n",
    "\ttextual_dataframe = pd.read_csv(abstracts_log_name+'.txt', delimiter=\"\\t\")\n",
    "\tdata_reader_end_status_key = abstracts_log_name+\".txt has been ported to dataframe\"\t\n",
    "\tstatus_logger(status_logger_name, data_reader_end_status_key)\n",
    "\treturn textual_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the abstracts and preparing a list of abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_data_trimmer(textual_dataframe, status_logger_name):\n",
    "\ttextual_data_trimmer_start_status_key = \"Trimming data and preparing list of words\"\n",
    "\tstatus_logger(status_logger_name, textual_data_trimmer_start_status_key)\n",
    "\n",
    "\t'''This function converts the textual data into a list and removes special characters, virtue of email correspondence'''\n",
    "\ttextual_data = textual_dataframe.values.tolist()\n",
    "\tprint(textual_data)\n",
    "\n",
    "\ttextual_data_trimmer_end_status_key = \"Trimmed data and prepared list of words\"\n",
    "\tstatus_logger(status_logger_name, textual_data_trimmer_end_status_key)\n",
    "\treturn textual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the words here and also removing punctuations from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(textual_data, status_logger_name):\n",
    "\tsent_to_words_start_status_key = \"Tokenizing words\"\n",
    "\tstatus_logger(status_logger_name, sent_to_words_start_status_key)\n",
    "\t'''This function tokenizes each sentence into individual words; also called tokens'''\n",
    "\tfor sentence in textual_data:\n",
    "\t\tyield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\ttextual_data = list(sent_to_words(textual_data))\n",
    "\tsent_to_words_end_status_key = \"Tokenized words\"\n",
    "\tstatus_logger(status_logger_name, sent_to_words_end_status_key)\t\n",
    "\treturn textual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating ***```bigrams```*** from the ```textual_data``` that we have prepared from the abstracts. **Bigrams** are words that occur together, multiple times in a given body of text. For example, *torrential* and *rain* occur together more often with each other, as opposed to other adjective-noun pairs. Hence the two would form a bigram here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_generator(textual_data, status_logger_name):\n",
    "\tbigram_generator_start_status_key = \"Generating word bigrams\"\n",
    "\tstatus_logger(status_logger_name, bigram_generator_start_status_key)\t\n",
    "\t'''Takes the textual data and prepares the bigram, two collectively high frequency words'''\n",
    "\tbigram = gensim.models.Phrases(textual_data, min_count=5, threshold=100)\n",
    "\tbigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\tbigram_generator_end_status_key = \"Generated word bigrams\"\n",
    "\tstatus_logger(status_logger_name, bigram_generator_end_status_key)\t\n",
    "\treturn bigram_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the ```bigram_mod``` model to generate bigrams from our ```corpus```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(textual_data, status_logger_name):\n",
    "\t'''Generates multiple bigrams of word pairs in phrases that commonly occuring with each other over the corpus'''\n",
    "\tmake_bigrams_start_status_key = \"Generating bigrams\"\n",
    "\tstatus_logger(status_logger_name, make_bigrams_start_status_key)\n",
    "\n",
    "\tbigram_mod = bigram_generator(textual_data, status_logger_name)\n",
    "\treturn [bigram_mod[doc] for doc in textual_data]\n",
    "\n",
    "\tmake_bigrams_end_status_key = \"Generated bigrams\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the stopwords, as specified in a list at the start of the program. There's a marked improvement in the topics and keywords produced after expanding the cache of the words to be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(textual_data, status_logger_name):\n",
    "\t'''This function removes the standard set of stopwords from the corpus of abstract words'''\n",
    "\tremove_stopwords_start_status_key = \"Removing stopwords\"\n",
    "\tstatus_logger(status_logger_name, remove_stopwords_start_status_key)\n",
    "\treturn [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in textual_data]\n",
    "\tremove_stopwords_end_status_key = \"Removed stopwords\"\n",
    "\tstatus_logger(status_logger_name, remove_stopwords_end_status_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the ```pyLDAvis``` visualization from the ```lda_model``` generated and trained on our ```corpus``` & ```id2word``` dictionary. The time taken here is proportional to the size of the ```corpus```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizer_generator(lda_model, corpus, id2word, logs_folder_name, status_logger_name):\n",
    "\t'''This code generates the .html file with generates the visualization of the data prepared.'''\n",
    "\tvisualizer_generator_start_status_key = \"Preparing the topic modeling visualization\"\n",
    "\tstatus_logger(status_logger_name, visualizer_generator_start_status_key)\n",
    "    \n",
    "\ttextual_data_visualization = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "\tpyLDAvis.save_html(textual_data_visualization, session_folder_name+'/'+\"Data_Visualization_Topic_Modelling.html\")\n",
    "\n",
    "\tvisualizer_generator_end_status_key = \"Prepared the topic modeling visualization\"+\" \"+\"Data_Visualization_Topic_Modelling.html\"\n",
    "\tstatus_logger(status_logger_name, visualizer_generator_end_status_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization involves truncating a term to its root term. For example *running -> run* is an example of the ```lemmatization``` process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(status_logger_name, textual_data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "\tlemmatization_start_status_key = \"Beginning lemmatization\"\n",
    "\tstatus_logger(status_logger_name, lemmatization_start_status_key)\n",
    "\t\"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "\ttexts_out = []\n",
    "\tnlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\tfor sent in textual_data:\n",
    "\t\tdoc = nlp(\" \".join(sent))\n",
    "\t\ttexts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\tlemmatization_end_status_key = \"Ending lemmatization\"\n",
    "\tstatus_logger(status_logger_name, lemmatization_end_status_key)\n",
    "\treturn texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the ```nlp_engine_main()``` function where all the previously defined functions come together and generate the ```pyLDAvis``` visualization for topic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_engine_main(abstracts_log_name, status_logger_name):\n",
    "\tnlp_engine_main_start_status_key = \"Initiating the NLP Engine\"\n",
    "\tstatus_logger(status_logger_name, nlp_engine_main_start_status_key)\n",
    "\n",
    "\t'''Extracts the data from the .txt file and puts them into a Pandas dataframe buckets'''\n",
    "\ttextual_dataframe = data_reader(abstracts_log_name, status_logger_name)\n",
    "\t'''Rids the symbols and special characters from the textual_data'''\n",
    "\ttextual_data = textual_data_trimmer(textual_dataframe, status_logger_name)\n",
    "\t'''Removes stopwords that were earlier downloaded from the textual_data'''\n",
    "\ttextual_data_no_stops = remove_stopwords(textual_data, status_logger_name)\n",
    "\t'''Prepares bigrams'''\n",
    "\ttextual_data_words_bigrams = make_bigrams(textual_data_no_stops, status_logger_name)\n",
    "\t'''Loads the English model from spaCy'''\n",
    "\tnlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "\ttextual_data_lemmatized = lemmatization(status_logger_name, textual_data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\tid2word = corpora.Dictionary(textual_data_lemmatized)\n",
    "\n",
    "\ttexts = textual_data_lemmatized\n",
    "\tcorpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\t[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "\t'''Builds the actual LDA model that will be used for the visualization and inference'''\n",
    "\tlda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = 20, random_state = 100, update_every = 1, chunksize = 100, passes = 10, alpha = 'auto', per_word_topics = True)\n",
    "\n",
    "\tdoc_lda = lda_model[corpus]\n",
    "\n",
    "\tperplexity_score = lda_model.log_perplexity(corpus)\n",
    "\n",
    "\tperplexity_status_key = \"Issued perplexity:\"+\" \"+str(perplexity_score)\n",
    "\n",
    "\tstatus_logger(status_logger_name, perplexity_status_key)\n",
    "    \n",
    "\tvisualizer_generator(lda_model, corpus, id2word, abstracts_log_name, status_logger_name)\n",
    "    \n",
    "\tnlp_engine_main_end_status_key = \"Idling the NLP Engine\"\n",
    "\tstatus_logger(status_logger_name, nlp_engine_main_end_status_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we trigger the entire workflow and prepare the topic modelling charts for specific text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]18:37:48 Initiating the NLP Engine\n",
      "[INFO]18:37:48 /home/sarthak/projects/Bias/BackUp_Journal_LOGS/Corpus_Bio_Hotspot_Data/JournalCorpusBiodiversity.txt is being ported to dataframe\n",
      "[INFO]18:37:48 /home/sarthak/projects/Bias/BackUp_Journal_LOGS/Corpus_Bio_Hotspot_Data/JournalCorpusBiodiversity.txt has been ported to dataframe\n",
      "[INFO]18:37:48 Trimming data and preparing list of words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]18:38:11 Generating bigrams\n",
      "[INFO]18:38:11 Generating word bigrams\n",
      "[INFO]18:38:35 Generated word bigrams\n",
      "[INFO]18:38:45 Beginning lemmatization\n",
      "[INFO]18:41:58 Ending lemmatization\n",
      "[INFO]18:56:20 Issued perplexity: -13.431836367695974\n",
      "[INFO]18:56:20 Preparing the topic modeling visualization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/.local/lib/python3.5/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]19:56:37 Prepared the topic modeling visualization Data_Visualization_Topic_Modelling.html\n",
      "[INFO]19:56:37 Idling the NLP Engine\n"
     ]
    }
   ],
   "source": [
    "nlp_engine_main(abstracts_log_name, status_logger_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
