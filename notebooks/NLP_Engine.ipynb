{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script can be used as a standalone program to process and prepare topic modelling of the ```Scraper.py``` datasets generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 <u>Code:</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import the libraries required to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sarthak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Natural Language toolkit. Here we download the commonly used English stopwords'''\n",
    "import nltk; nltk.download('stopwords')\n",
    "'''Standard set of functions for reading and appending files'''\n",
    "import re\n",
    "'''Pandas and numpy is a dependency used by other portions of the code.'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "'''Think this stands for pretty print. Prints out stuff to the terminal in a prettier way'''\n",
    "from pprint import pprint\n",
    "\n",
    "'''Contains the language model that has to be developed.'''\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "'''Industrial level toolkit for NLP'''\n",
    "import spacy\n",
    "\n",
    "'''Importing OS here to build the directories to hold the status logger logs'''\n",
    "import os\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "'''Importing datetime here to log the time when specfic functions are triggered'''\n",
    "from datetime import datetime\n",
    "\n",
    "'''Make pretty visualizations'''\n",
    "import matplotlib as plt\n",
    "\n",
    "'''Library to log any errors. Came across this in the tutorial.'''\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'com', 'https', 'url', 'link', 'xe', 'abstract', 'author', 'chapter', 'springer', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the abstracts log name that will be processed by the rest of the script. The status logger function of the main script has been imported here, and the name of the ```.txt``` file used to log the information has to be added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_log_name = \"/home/sarthak/projects/Bias/LOGS/LOG_2019-02-27_15_23_Eastern_Himalayas/\"+\"Abstract_Database_2019-02-27_15_23\"\n",
    "session_folder_name = abstracts_log_name.split('/')[6]\n",
    "os.makedirs(session_folder_name)\n",
    "status_logger_name = session_folder_name+\"/\"+\"Status_Logger\"+\"_\"+abstracts_log_name.split('/')[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is responsible for generating a ```.txt``` file for analysis once the code has run. This code has been copied directly from the ```common_functions.py``` script of the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_logger(status_logger_name, status_key):\n",
    "\t'''Status logger to print and log details throught the running the program.\n",
    "\tDeclaring current_hour, current_minute & current_second.'''\n",
    "\tcurrent_hour = str(datetime.now().time().hour)\n",
    "\tcurrent_minute = str(datetime.now().time().minute)\n",
    "\tcurrent_second = str(datetime.now().time().second)\n",
    "\t'''Logging the complete_status_key and printing the complete_status_key'''\n",
    "\tcomplete_status_key = \"[INFO]\"+current_hour+\":\"+current_minute+\":\"+current_second+\" \"+status_key\n",
    "\tprint(complete_status_key)\n",
    "\tstatus_log = open(status_logger_name+'.txt', 'a')\n",
    "\tstatus_log.write(complete_status_key+\"\\n\")\n",
    "\tstatus_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the different finctions required to carry out the actual topic modelling from the dataset generated from the runs of the ```Scraper.py``` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(abstracts_log_name, status_logger_name):\n",
    "\tdata_reader_start_status_key = abstracts_log_name+\".txt is being ported to dataframe\"\n",
    "\tstatus_logger(status_logger_name, data_reader_start_status_key)\n",
    "\ttextual_dataframe = pd.read_csv(abstracts_log_name+'_'+'ANALYTICAL'+'.txt', delimiter=\"\\t\")\n",
    "\tdata_reader_end_status_key = abstracts_log_name+\".txt has been ported to dataframe\"\t\n",
    "\tstatus_logger(status_logger_name, data_reader_end_status_key)\n",
    "\treturn textual_dataframe\n",
    "\n",
    "def textual_data_trimmer(textual_dataframe, status_logger_name):\n",
    "\ttextual_data_trimmer_start_status_key = \"Trimming data and preparing list of words\"\n",
    "\tstatus_logger(status_logger_name, textual_data_trimmer_start_status_key)\n",
    "\t'''This function converts the textual data into a list and removes special characters, virtue of email correspondence'''\n",
    "\ttextual_data = textual_dataframe.values.tolist()\n",
    "\ttextual_data_trimmer_end_status_key = \"Trimmed data and prepared list of words\"\n",
    "\tstatus_logger(status_logger_name, textual_data_trimmer_end_status_key)\n",
    "\treturn textual_data\n",
    "\n",
    "def sent_to_words(textual_data, status_logger_name):\n",
    "\tsent_to_words_start_status_key = \"Tokenizing words\"\n",
    "\tstatus_logger(status_logger_name, sent_to_words_start_status_key)\n",
    "\t'''This function tokenizes each sentence into individual words; also called tokens'''\n",
    "\tfor sentence in textual_data:\n",
    "\t\tyield(gensim.utils.simple_preprocess(str(textual_data), deacc=True))\n",
    "\ttextual_data = list(sent_to_words(textual_data))\n",
    "\tsent_to_words_end_status_key = \"Tokenized words\"\n",
    "\tstatus_logger(status_logger_name, sent_to_words_end_status_key)\t\n",
    "\treturn textual_data\n",
    "\n",
    "def bigram_generator(textual_data, status_logger_name):\n",
    "\tbigram_generator_start_status_key = \"Generating word bigrams\"\n",
    "\tstatus_logger(status_logger_name, bigram_generator_start_status_key)\t\n",
    "\t'''Takes the textual data and prepares the bigram, two collectively high frequency words'''\n",
    "\tbigram = gensim.models.Phrases(textual_data, min_count=5, threshold=100)\n",
    "\tbigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\tbigram_generator_end_status_key = \"Generated word bigrams\"\n",
    "\tstatus_logger(status_logger_name, bigram_generator_end_status_key)\t\n",
    "\treturn bigram_mod\n",
    "\n",
    "def trigram_generator(textual_data, status_logger_name):\n",
    "\t'''Takes the textual data and prepares the trigram, three collectively high frequency words'''\n",
    "\ttrigram_generator_start_status_key = \"Generating word trigrams\"\n",
    "\tstatus_logger(status_logger_name, trigram_generator_start_status_key)\n",
    "\ttrigram = gensim.models.Phrases(bigram[textual_data], threshold=100)\n",
    "\ttrigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\tprint(\"Printing the trigram_mod\\n\")\n",
    "\tpprint(trigram_mod[bigram_mod[textual_data[0]]])\n",
    "\tprint(\"Print of the trigram_mod has concluded\\n\")\n",
    "\ttrigram_generator_end_status_key = \"Generating word trigrams\"\n",
    "\tstatus_logger(status_logger_name, trigram_generator_end_status_key)\n",
    "\treturn trigram_mod\n",
    "\n",
    "def remove_stopwords(textual_data, status_logger_name):\n",
    "\t'''This function removes the standard set of stopwords from the corpus of abstract words'''\n",
    "\tremove_stopwords_start_status_key = \"Removing stopwords\"\n",
    "\tstatus_logger(status_logger_name, remove_stopwords_start_status_key)\n",
    "\treturn [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in textual_data]\n",
    "\tremove_stopwords_end_status_key = \"Removed stopwords\"\n",
    "\tstatus_logger(status_logger_name, remove_stopwords_end_status_key)\n",
    "\n",
    "def make_bigrams(textual_data, status_logger_name):\n",
    "\t'''Generates multiple bigrams of word pairs in phrases that commonly occuring with each other over the corpus'''\n",
    "\tmake_bigrams_start_status_key = \"Generating bigrams\"\n",
    "\tstatus_logger(status_logger_name, make_bigrams_start_status_key)\n",
    "\n",
    "\tbigram_mod = bigram_generator(textual_data, status_logger_name)\n",
    "\treturn [bigram_mod[doc] for doc in textual_data]\n",
    "\t\n",
    "\tmake_bigrams_end_status_key = \"Generated bigrams\"\n",
    "\tstatus_logger(status_logger_name, make_bigrams_end_status_key)\n",
    "\n",
    "def make_trigram(textual_data, status_logger_name):\n",
    "\t'''Generates multiple trigrams of triplet words in phrases that commonly occuring with each other over the abstract corpus'''\n",
    "\tmake_trigrams_start_status_key = \"Generating trigrams\"\n",
    "\tstatus_logger(status_logger_name, make_trigrams_start_status_key)\n",
    "\n",
    "\ttrigram_mod = trigram_generator(textual_data)\n",
    "\treturn [trigram_mod[bigram_mod[doc]] for doc in textual_data]\n",
    "\t\n",
    "\tmake_trigrams_end_status_key = \"Generated trigrams\"\n",
    "\tstatus_logger(status_logger_name, make_trigrams_end_status_key)\n",
    "\n",
    "\n",
    "def visualizer_generator(lda_model, corpus, id2word, logs_folder_name, status_logger_name):\n",
    "\t'''This code generates the .html file with generates the visualization of the data prepared.'''\n",
    "\tvisualizer_generator_start_status_key = \"Preparing the topic modeling visualization\"\n",
    "\tstatus_logger(status_logger_name, visualizer_generator_start_status_key)\n",
    "\ttextual_data_visualization = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "\tpyLDAvis.save_html(textual_data_visualization, session_folder_name+'/'+\"Data_Visualization_Topic_Modelling.html\")\n",
    "\tvisualizer_generator_end_status_key = \"Prepared the topic modeling visualization\"+\" \"+\"Data_Visualization_Topic_Modelling.html\"\n",
    "\tstatus_logger(status_logger_name, visualizer_generator_end_status_key)\n",
    "    \n",
    "def lemmatization(status_logger_name, textual_data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "\tlemmatization_start_status_key = \"Beginning lemmatization\"\n",
    "\tstatus_logger(status_logger_name, lemmatization_start_status_key)\n",
    "\t\"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "\ttexts_out = []\n",
    "\tnlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\tfor sent in textual_data:\n",
    "\t\tdoc = nlp(\" \".join(sent))\n",
    "\t\ttexts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\tlemmatization_end_status_key = \"Ending lemmatization\"\n",
    "\tstatus_logger(status_logger_name, lemmatization_end_status_key)\n",
    "\treturn texts_out\n",
    "\n",
    "def nlp_engine_main(abstracts_log_name, status_logger_name):\n",
    "\tnlp_engine_main_start_status_key = \"Initiating the NLP Engine\"\n",
    "\tstatus_logger(status_logger_name, nlp_engine_main_start_status_key)\n",
    "\n",
    "\t'''Extracts the data from the .txt file and puts them into a Pandas dataframe buckets'''\n",
    "\ttextual_dataframe = data_reader(abstracts_log_name, status_logger_name)\n",
    "\t'''Rids the symbols and special characters from the textual_data'''\n",
    "\ttextual_data = textual_data_trimmer(textual_dataframe, status_logger_name)\n",
    "\t'''Removes stopwords that were earlier downloaded from the textual_data'''\n",
    "\ttextual_data_no_stops = remove_stopwords(textual_data, status_logger_name)\n",
    "\t'''Prepares bigrams'''\n",
    "\ttextual_data_words_bigrams = make_bigrams(textual_data_no_stops, status_logger_name)\n",
    "\t'''Loads the English model from spaCy'''\n",
    "\tnlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "\ttextual_data_lemmatized = lemmatization(status_logger_name, textual_data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\tid2word = corpora.Dictionary(textual_data_lemmatized)\n",
    "\n",
    "\ttexts = textual_data_lemmatized\n",
    "\tcorpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\t[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "\t'''Builds the actual LDA model that will be used for the visualization and inference'''\n",
    "\tlda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = 10, random_state = 100, update_every = 1, chunksize = 100, passes = 10, alpha = 'auto', per_word_topics = True)\n",
    "\n",
    "\tdoc_lda = lda_model[corpus]\n",
    "\n",
    "\tperplexity_score = lda_model.log_perplexity(corpus)\n",
    "\n",
    "\tperplexity_status_key = \"Issued perplexity:\"+\" \"+str(perplexity_score)\n",
    "\n",
    "\tstatus_logger(status_logger_name, perplexity_status_key)\n",
    "    \n",
    "\tvisualizer_generator(lda_model, corpus, id2word, abstracts_log_name, status_logger_name)\n",
    "    \n",
    "\tnlp_engine_main_end_status_key = \"Idling the NLP Engine\"\n",
    "\tstatus_logger(status_logger_name, nlp_engine_main_end_status_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]15:28:56 Initiating the NLP Engine\n",
      "[INFO]15:28:56 /home/sarthak/projects/Bias/LOGS/LOG_2019-02-27_15_23_Eastern_Himalayas/Abstract_Database_2019-02-27_15_23.txt is being ported to dataframe\n",
      "[INFO]15:28:56 /home/sarthak/projects/Bias/LOGS/LOG_2019-02-27_15_23_Eastern_Himalayas/Abstract_Database_2019-02-27_15_23.txt has been ported to dataframe\n",
      "[INFO]15:28:56 Trimming data and preparing list of words\n",
      "[INFO]15:28:56 Trimmed data and prepared list of words\n",
      "[INFO]15:28:56 Removing stopwords\n",
      "[INFO]15:29:3 Generating bigrams\n",
      "[INFO]15:29:3 Generating word bigrams\n",
      "[INFO]15:29:20 Generated word bigrams\n",
      "[INFO]15:29:26 Beginning lemmatization\n",
      "[INFO]15:30:44 Ending lemmatization\n",
      "[INFO]15:32:40 Issued perplexity: -8.977581719192687\n",
      "[INFO]15:32:40 Preparing the topic modeling visualization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/.local/lib/python3.5/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]16:8:56 Prepared the topic modeling visualization Data_Visualization_Topic_Modelling.html\n",
      "[INFO]16:8:56 Idling the NLP Engine\n"
     ]
    }
   ],
   "source": [
    "nlp_engine_main(abstracts_log_name, status_logger_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
