{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is part of the broader breakdown of the [Bias](https://github.com/SarthakJShetty/Bias) project. This script cleans up the ```abstract_word_list()``` pre-processing and is a stripped down version of the [Scraper.py](https://github.com/SarthakJShetty/Bias/blob/master/Scraper.py) code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 <u>Code:</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the libraries that will be used across the code. ```numpy``` is used to generate the arrays for the ```pandas``` DataFrame. ```status_logger()``` function has been moved here as well, over from the ```common_functions``` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing OS here to split the filename at the extension'''\n",
    "import os\n",
    "'''Importing the collections which contains the Counter function'''\n",
    "from collections import Counter\n",
    "'''Importing pandas here to build the dataframe'''\n",
    "import pandas as pd\n",
    "'''Importing datetime library to generate the log files by the status_logger() function'''\n",
    "from datetime import datetime\n",
    "'''Importing numpy here to build the index of the pandas frameword'''\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the required variables here, including the ```status_logger_name``` and the ```pwd``` of the ```abstracts_log_name```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_log_name = \"/home/sarthak/projects/Bias/LOGS/LOG_2019-02-27_15_23_Eastern_Himalayas/Abstract_Database_2019-02-27_15_23\"\n",
    "log_name = abstracts_log_name.split('/')\n",
    "status_logger_name = log_name[6]+\"_\"+'Status_Logger'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the ```status_logger()``` function here which logs the status keys of the different functions for trouble-shooting in case of uneventful run of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_logger(status_logger_name, status_key):\n",
    "\t'''Status logger to print and log details throught the running the program.\n",
    "\tDeclaring current_hour, current_minute & current_second.'''\n",
    "\tcurrent_hour = str(datetime.now().time().hour)\n",
    "\tcurrent_minute = str(datetime.now().time().minute)\n",
    "\tcurrent_second = str(datetime.now().time().second)\n",
    "\n",
    "\t'''Logging the complete_status_key and printing the complete_status_key'''\n",
    "\tcomplete_status_key = \"[INFO]\"+current_hour+\":\"+current_minute+\":\"+current_second+\" \"+status_key\n",
    "\tprint(complete_status_key)\n",
    "\tstatus_log = open(status_logger_name+'.txt', 'a')\n",
    "\tstatus_log.write(complete_status_key+\"\\n\")\n",
    "\tstatus_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for the script are defined here, including ```analyzer_pre_processing()``` function which generates the ```.txt``` and the ```.csv``` files for holding the data in ```.csv``` buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_pre_processing(abstracts_log_name, status_logger_name):\n",
    "\t'''Carries out the pre-processing tasks, such as folder creation'''\n",
    "\tanalyzer_pre_processing_status_key=\"Carrying out pre-processing functions for analyzer\"\n",
    "\tstatus_logger(status_logger_name, analyzer_pre_processing_status_key)\n",
    "    \n",
    "\t'''This code strips the abstracts_log_name of its extension and adds a .csv to it'''\n",
    "\tabstracts_csv_file_name=(log_name[6])+\"_\"+\"FREQUENCY_CSV_DATA\"+\".csv\"\n",
    "\tabstracts_txt_file_name = abstracts_log_name+\"_\"+\"ANALYTICAL\"+\".txt\"\n",
    "\t\n",
    "\tanalyzer_pre_processing_status_key = \"Carried out pre-processing functions for analyzer\"\n",
    "\tstatus_logger(status_logger_name, analyzer_pre_processing_status_key)\n",
    "    \n",
    "\treturn abstracts_txt_file_name, abstracts_csv_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```list_cleaner()``` function contains a list of words scrapped as meta-data along with the main data of interest. Improves the topic modelling results and prevents cluttering of the topical spheres generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cleaner(list_to_be_cleaned, status_logger_name):\n",
    "\tlist_cleaner_start_status_key = \"Cleaning the list of words generated\"\n",
    "\tstatus_logger(status_logger_name, list_cleaner_start_status_key)\n",
    "    \n",
    "\t'''This function cleans the list containing the words found in the abstract. It eliminates words found in\n",
    "\tanother pre-defined list of words.'''\n",
    "\twords_to_be_eliminated = [\"the\", \"of\", \"and\", \"in\", \"to\", \"a\", \"is\", \"for\", \"from\", \"with\", \"that\",\t\"by\", \"are\", \"on\", \"was\", \"as\", \"were\", \"url:\", \"abstract:\",\n",
    "\t\"abstract\",  \"author:\", \"title:\", \"at\", \"be\", \"an\", \"during\", \"have\", \"this\", \"which\", \"study\", \"been\", \"species\", \"not\", \"has\", \"between\",\n",
    "\t\"using\", \"its\", \"also\", \"these\", \"this\", \"used\", \"over\", \"can\", \"within\", \"into\", \"all\",\"due\", \"use\", \"about\", \"a\", 'it', 'their', \"where\", \"we\", \"most\", \"may\", \"through\",\n",
    "\t\"though\", \"like\", \"or\", \"further\", \"e.g.\", \"along\", \"any\", \"those\", \"had\", \"toward\", \"due\", \"both\", \"some\", \"use\", \"even\", \"more\", \"but\", \"while\", \"pass\", \n",
    "\t\"well\", \"will\", \"when\", \"only\", \"after\", \"author\", \"title\", \"there\", \"our\", \"did\", \"much\", \"as\", \"if\", \"become\", \"still\", \"various\", \"very\", \"out\",\n",
    "\t\"they\", \"via\", \"available\", \"such\", \"than\", \"different\", \"many\", \"areas\", \"no\", \"one\", \"two\", \"small\", \"first\", \"other\", \"such\", \"-\", \"could\", \"studies\", \"high\",\n",
    "\t\"provide\", \"among\", \"highly\", \"no\", \"case\", \"across\", \"given\", \"need\", \"would\", \"under\", \"found\", \"low\", \"values\", \"xe2\\\\x80\\\\x89\", \"xa\", \"xc\", \"xb\", \"\\xc2\\xa0C\\xc2\\xa0ha\\xe2\\x88\\x921\", \"suggest\", \"up\", \"'The\", \"area\"] \n",
    "\tcleaned_list_of_words_in_abstract = [item for item in list_to_be_cleaned if item not in words_to_be_eliminated]\n",
    "\n",
    "\tlist_cleaner_end_status_key = \"Cleaned the list of words generated\"\n",
    "\tstatus_logger(status_logger_name, list_cleaner_end_status_key)\n",
    "\n",
    "\treturn cleaned_list_of_words_in_abstract\n",
    "\n",
    "def transfer_function(abstracts_txt_file_name, abstracts_csv_file_name, status_logger_name):\n",
    "\t'''This function is involved in the actual transfer of data from the .txt file to the .csv file'''\n",
    "\ttransfer_function_status_key = \"Copying data from\"+\" \"+str(abstracts_txt_file_name)+\" \"+\"to\"+\" \"+\"pandas dataframe\"\n",
    "\tstatus_logger(status_logger_name, transfer_function_status_key)\n",
    "\n",
    "\t'''This list will contain all the words extracted from the .txt abstract file'''\n",
    "\tlist_of_words_in_abstract=[]\n",
    "\n",
    "\t'''Each word is appended to the list, from the .txt file'''\n",
    "\twith open(abstracts_txt_file_name, 'r') as abstracts_txt_data:\n",
    "\t\tfor line in abstracts_txt_data:\n",
    "\t\t\tfor word in line.split():\n",
    "\t\t\t\tlist_of_words_in_abstract.append(word)\n",
    "\n",
    "\t'''This function cleans up the data of uneccessary words'''\n",
    "\tcleaned_list_of_words_in_abstract = list_cleaner(list_of_words_in_abstract, status_logger_name)\n",
    "\n",
    "\t'''A Counter is a dictionary, where the value is the frequency of term, which is the key'''\n",
    "\tdictionary_of_abstract_list = Counter(cleaned_list_of_words_in_abstract)\n",
    "\n",
    "\tlength_of_abstract_list = len(dictionary_of_abstract_list)\n",
    "\n",
    "\t'''Building a dataframe to hold the data from the list, which in turn contains the data from '''\n",
    "\tdataframe_of_abstract_words=pd.DataFrame(index=np.arange(0, length_of_abstract_list), columns=['Words', 'Frequency'])\n",
    "\n",
    "\t'''An element to keep tab of the number of elements being added to the list'''\n",
    "\tdictionary_counter = 0\n",
    "\n",
    "\t'''Copying elements from the dictionary to the pandas file'''\n",
    "\tfor dictionary_element in dictionary_of_abstract_list:\n",
    "\t\tif(dictionary_counter==length_of_abstract_list):\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tdataframe_of_abstract_words.loc[dictionary_counter, 'Words'] = dictionary_element\n",
    "\t\t\tdataframe_of_abstract_words.loc[dictionary_counter, 'Frequency'] = dictionary_of_abstract_list[dictionary_element]\n",
    "\t\t\tdictionary_counter = dictionary_counter+1\n",
    "\n",
    "\ttransfer_function_status_key = \"Copied data from\"+\" \"+str(abstracts_txt_file_name)+\" \"+\"to\"+\" \"+\"pandas dataframe\"\n",
    "\tstatus_logger(status_logger_name, transfer_function_status_key)\n",
    "\n",
    "\ttransfer_function_status_key = \"Copying data from pandas dataframe to\"+\" \"+str(abstracts_csv_file_name)\n",
    "\tstatus_logger(status_logger_name, transfer_function_status_key)\n",
    "\n",
    "\t'''Saving dataframe to csv file, without the index column'''\n",
    "\tdataframe_of_abstract_words.to_csv(abstracts_csv_file_name, index=False)\n",
    "\n",
    "\ttransfer_function_status_key = \"Copied data from pandas dataframe to\"+\" \"+str(abstracts_csv_file_name)\n",
    "\tstatus_logger(status_logger_name, transfer_function_status_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the ```analyzer_main()``` function responsible for executing all functions of the Analyzer script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_main(abstracts_log_name, status_logger_name):\n",
    "\t'''Declaring the actual analyzer_main function is integrated to Bias.py code'''\n",
    "\tanalyzer_main_status_key=\"Entered the Analyzer.py code.\"\n",
    "\tstatus_logger(status_logger_name, analyzer_main_status_key)\n",
    "\n",
    "\t'''Calling the pre-processing and transfer functions here'''\n",
    "\tabstracts_txt_file_name, abstracts_csv_file_name = analyzer_pre_processing(abstracts_log_name, status_logger_name)\n",
    "\ttransfer_function(abstracts_txt_file_name, abstracts_csv_file_name, status_logger_name)\n",
    "    \n",
    "\t'''Logs the end of the process Analyzer code in the status_logger'''\n",
    "\tanalyzer_main_status_key=\"Exiting the Analyzer.py code.\"\n",
    "\tstatus_logger(status_logger_name, analyzer_main_status_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the ```analyzer_main()``` function here to execute the script and generate the cleaned abstracts data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]10:33:5 Entered the Analyzer.py code.\n",
      "[INFO]10:33:5 Carrying out pre-processing functions for analyzer\n",
      "[INFO]10:33:5 Carried out pre-processing functions for analyzer\n",
      "[INFO]10:33:5 Copying data from /home/sarthak/projects/Bias/LOGS/LOG_2019-02-27_15_23_Eastern_Himalayas/Abstract_Database_2019-02-27_15_23_ANALYTICAL.txt to pandas dataframe\n",
      "[INFO]10:33:5 Cleaning the list of words generated\n",
      "[INFO]10:33:7 Cleaned the list of words generated\n",
      "[INFO]10:47:22 Copied data from /home/sarthak/projects/Bias/LOGS/LOG_2019-02-27_15_23_Eastern_Himalayas/Abstract_Database_2019-02-27_15_23_ANALYTICAL.txt to pandas dataframe\n",
      "[INFO]10:47:22 Copying data from pandas dataframe to LOG_2019-02-27_15_23_Eastern_Himalayas_FREQUENCY_CSV_DATA.csv\n",
      "[INFO]10:47:22 Copied data from pandas dataframe to LOG_2019-02-27_15_23_Eastern_Himalayas_FREQUENCY_CSV_DATA.csv\n",
      "[INFO]10:47:22 Exiting the Analyzer.py code.\n"
     ]
    }
   ],
   "source": [
    "analyzer_main(abstracts_log_name, status_logger_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
