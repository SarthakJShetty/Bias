{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is a stripped down version of the [Scraper.py](https://github.com/SarthakJShetty/Bias/blob/master/Scraper.py) code that is used to generate the dataset for analyses of research trends. It's a small piece of the larger [Bias](https://GitHub.com/SarthakJShetty/Bias) project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 <u>Code</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are importing the libraries required to run the scraping code. We are retrieving the abstacts, paper title and author names from [Springer](https://link.springer.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "''''Importing urllib.error to handle errors in HTTP pinging.'''\n",
    "import urllib.error\n",
    "'''BeautifulSoup is used for souping.'''\n",
    "from bs4 import BeautifulSoup\n",
    "'''Counter generates a dictionary from the abstract data, providing frequencies of occurences'''\n",
    "from collections import Counter\n",
    "'''Importing the CSV library here to dump the dictionary for further analysis and error checking if required. Will edit it out later.'''\n",
    "import csv\n",
    "'''Importing datetime for logging of various function executions'''\n",
    "from datetime import datetime\n",
    "'''Importing numpy to generate a random integer for the delay_function (see below)'''\n",
    "import numpy as np\n",
    "'''This library is imported to check if we can feasibly introduce delays into the processor loop to reduce instances of the remote server, shutting the connection while scrapping extraordinarily large datasets.'''\n",
    "import time\n",
    "'''This library is imported to build the LOG folders and directories'''\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring the ```keywords``` variable here, which is topic that the scrapping will take place on. It will be appended to the start and abstract URLs during retrival. Also defining the ```trend_keywords``` term here, whose frequency is to be studied using the trends chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_to_search=\"Biodiversity and Conservation\"\n",
    "keywords_to_search = keywords_to_search.split()\n",
    "trend_keywords=\"Conservation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we define the functions that are responsible for pinging, retriving and storing the information and the meta-data collected from the Springer scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(keywords):\n",
    "\t'''This function contains all the pre-processing statements related to the running of the program, including:\n",
    "\t1. Default starter URL of the page to scrape.\n",
    "\t2. Default location of LOGS, including:\n",
    "\t\t1. Location of Abstract_ID_Database\n",
    "\t\t2. Location of Abstract_Database'''\n",
    "\n",
    "\t'''Declaring the time and date variables here. Year, month, day, hours, minute & seconds.'''\n",
    "\trun_start_year = str(datetime.now().date().year)\n",
    "\trun_start_month = str(datetime.now().date().month)\n",
    "\trun_start_day = str(datetime.now().date().day)\n",
    "\trun_start_date = str(datetime.now().date())\n",
    "\trun_start_hour = str(datetime.now().time().hour)\n",
    "\trun_start_minute = str(datetime.now().time().minute)\n",
    "\trun_start_second = str(datetime.now().time().second)\n",
    "\t'''Keywords have to be written into the filename of the LOG that we are running'''\n",
    "\tfolder_attachement = \"\"\n",
    "\tif(len(keywords)==1):\n",
    "\t\tfolder_attachement = keywords[0]\n",
    "\telse:\n",
    "\t\tfor keyword_index in range(0, len(keywords)):\n",
    "\t\t\tif((keyword_index+1)==len(keywords)):\n",
    "\t\t\t\tfolder_attachement = folder_attachement+keywords[keyword_index]\n",
    "\t\t\telse:\n",
    "\t\t\t\tfolder_attachement = folder_attachement+keywords[keyword_index]+\"_\"\n",
    "\t'''Declaring the LOG folder and the abstract, abstract_id & status_logger files.'''\n",
    "\tlogs_folder_name = \"LOGS\"+\"/\"+\"LOG\"+\"_\"+run_start_date+'_'+run_start_hour+'_'+run_start_minute+\"_\"+folder_attachement\n",
    "\tabstract_id_log_name =logs_folder_name+\"/\"+'Abstract_ID_Database'+'_'+run_start_date+'_'+run_start_hour+'_'+run_start_minute+\"_\"\n",
    "\tabstracts_log_name = logs_folder_name+\"/\"+'Abstract_Database'+'_'+run_start_date+'_'+run_start_hour+'_'+run_start_minute\n",
    "\tstatus_logger_name = logs_folder_name+\"/\"+'Status_Logger'+'_'+run_start_date+'_'+run_start_hour+'_'+run_start_minute\n",
    "\n",
    "\t'''If the filename does not exist create the file in the LOG directory'''\n",
    "\tif not os.path.exists(logs_folder_name):\n",
    "\t\tos.makedirs(logs_folder_name)\n",
    "\t\n",
    "\t'''Creating the status_log and writing the session duration & date'''\n",
    "\tstatus_log = open(status_logger_name+'.txt', 'a')\n",
    "\tstatus_log.write(\"Session:\"+\" \"+run_start_day+\"/\"+run_start_month+\"/\"+run_start_year+\"\\n\")\n",
    "\tstatus_log.write(\"Time:\"+\" \"+run_start_hour+\":\"+run_start_minute+\":\"+run_start_second+\"\\n\")\n",
    "\tstatus_log.close()\n",
    "\n",
    "\tlogs_folder_name_status_key = \"Built LOG folder for session\"\n",
    "\tstatus_logger(status_logger_name, logs_folder_name_status_key)\n",
    "\n",
    "\tquery_string = \"\"\n",
    "\tif (len(keywords)==1):\n",
    "\t\tquery_string = keywords[0]\n",
    "\telse:\n",
    "\t\tfor keyword_index in range(0, len(keywords)):\n",
    "\t\t\tif((keyword_index+1)==len(keywords)):\n",
    "\t\t\t\tquery_string = query_string+keywords[keyword_index]\n",
    "\t\t\telse:\n",
    "\t\t\t\tquery_string = query_string+keywords[keyword_index]+\"+\"\n",
    "\n",
    "\tstart_url = \"https://link.springer.com/search/page/\"\n",
    "\tabstract_url = 'https://link.springer.com'\n",
    "\n",
    "\treturn abstract_id_log_name, abstracts_log_name, start_url, abstract_url, query_string, logs_folder_name, status_logger_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the ```status_logger()``` function here which will log the functioning of all the individual modules. Primarily designed to improve diagnostics of the code once the scrapping has commenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_logger(status_logger_name, status_key):\n",
    "\t'''Status logger to print and log details throught the running the program.\n",
    "\tDeclaring current_hour, current_minute & current_second.'''\n",
    "\tcurrent_hour = str(datetime.now().time().hour)\n",
    "\tcurrent_minute = str(datetime.now().time().minute)\n",
    "\tcurrent_second = str(datetime.now().time().second)\n",
    "\t'''Logging the complete_status_key and printing the complete_status_key'''\n",
    "\tcomplete_status_key = \"[INFO]\"+current_hour+\":\"+current_minute+\":\"+current_second+\" \"+status_key\n",
    "\tprint(complete_status_key)\n",
    "\tstatus_log = open(status_logger_name+'.txt', 'a')\n",
    "\tstatus_log.write(complete_status_key+\"\\n\")\n",
    "\tstatus_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```pre_processing()``` function is responsible for setting up the fundamental files and variables required to get the scrapper up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_id_log_name, abstracts_log_name, start_url, abstract_url, query_string, logs_folder_name, status_logger_name = pre_processing(keywords_to_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond this block, the actual functions powering the script begin. Comments are provided at the beginning of each function to ensure readability and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_reader(url, status_logger_name):\n",
    "\t'''This keyword is supplied to the URL and is hence used for souping.\n",
    "\tEncountered an error where some links would not open due to HTTP.error\n",
    "\tThis is added here to try and ping the page. If it returns false the loop ignores it and\n",
    "\tmoves on to the next PII number'''\n",
    "\ttry:\n",
    "\t\tpage=urlopen(url)\n",
    "\t\tpage_status(page, status_logger_name)\n",
    "\t\treturn page\n",
    "\texcept (UnboundLocalError, urllib.error.HTTPError):\n",
    "\t\tpass\n",
    "\n",
    "def results_determiner(url, status_logger_name):\n",
    "\t'''This function determines the number of results that a particular keywords returns\n",
    "\tonce it looks up the keyword on link.springer.com\n",
    "\tThe function returns all the possible links containing results and then provides the total number of results\n",
    "\treturned by a particular keyword, or combination of keywords.'''\n",
    "\tfirst_page_to_scrape = urlopen(url)\n",
    "\tfirst_page_to_scrape_soup = BeautifulSoup(first_page_to_scrape, 'html.parser')\n",
    "\tnumber_of_results = first_page_to_scrape_soup.find('h1', {'id':'number-of-search-results-and-search-terms'}).find('strong').text\n",
    "\n",
    "def url_generator(start_url, query_string, status_logger_name):\n",
    "\t'''This function is written to scrape all possible webpages of a given topic\n",
    "\tThe search for the URLs truncates when determiner variable doesn't return a positive value'''\n",
    "\t'''Initiallizing a list here in order to contain the URLs. Even if a URL does not return valid results,\n",
    "\tit is popped later on from the list.'''\n",
    "\turls_to_scrape=[]\n",
    "\tcounter = 0\n",
    "\ttotal_url = start_url+str(counter)+\"?facet-content-type=\\\"Article\\\"&query=\"+query_string\n",
    "\tinitial_url_status_key = total_url+\" \"+\"has been obtained\"\n",
    "\tstatus_logger(status_logger_name, initial_url_status_key)\n",
    "\turls_to_scrape.append(total_url)\n",
    "\ttest_soup = BeautifulSoup(urlopen(total_url), 'html.parser')\n",
    "\tdeterminer = test_soup.find('a', {'class':'title'})\n",
    "\t'''This while loop continuously pings and checks for new webpages, then stores them for scraping'''\n",
    "\twhile(determiner):\n",
    "\t\tcounter = counter+1\n",
    "\t\ttotal_url = start_url+str(counter)+\"?facet-content-type=\\\"Article\\\"&query=\"+query_string\n",
    "\t\turl_generator_while_status_key=total_url+\" \"+\"has been obtained\"\n",
    "\t\tstatus_logger(status_logger_name, url_generator_while_status_key)\n",
    "\t\tsoup = BeautifulSoup(urlopen(total_url), 'html.parser')\n",
    "\t\tdeterminer = soup.find('a', {'class':'title'})\n",
    "\t\turls_to_scrape.append(total_url)\n",
    "\turls_to_scrape.pop(len(urls_to_scrape)-1)\n",
    "\treturn urls_to_scrape\n",
    "\n",
    "def page_status(page, status_logger_name):\n",
    "\t'''Prints the page status. Will be used whenever a new webpage is picked for scraping.'''\n",
    "\tpage_status_key = \"Page status:\"+\" \"+str(page.status)\n",
    "\tstatus_logger(status_logger_name, page_status_key)\n",
    "\n",
    "def page_souper(page, status_logger_name):\n",
    "\t'''Function soups the webpage elements and provided the tags for search.\n",
    "\tNote: Appropriate encoding has to be picked up beenfore souping'''\n",
    "\tpage_souper_start_status_key = \"Souping page\"\n",
    "\tstatus_logger(status_logger_name, page_souper_start_status_key)\n",
    "\tpage_soup = BeautifulSoup(page, 'html.parser')\n",
    "\tpage_souper_stop_status_key = \"Souped page\"\n",
    "\tstatus_logger(status_logger_name, page_souper_stop_status_key)\n",
    "\treturn page_soup\n",
    "\n",
    "def abstract_word_extractor(abstract, abstract_title, abstract_year, permanent_word_sorter_list, trend_keywords, status_logger_name):\n",
    "\t'''This function creates the list that stores the text in the form of individual words\n",
    "\tagainst their year of appearence.'''\n",
    "\tabstract_word_sorter_start_status_key = \"Adding:\"+\" \"+abstract_title+\" \"+\"to the archival list\"\n",
    "\tstatus_logger(status_logger_name, abstract_word_sorter_start_status_key)\n",
    "\t'''This line of code converts the entire abstract into lower case'''\n",
    "\tabstract = abstract.lower()\n",
    "\t'''Converting the abstract into a list of words'''\n",
    "\tabstract_word_list = abstract.split()\n",
    "\t'''This line of code sorts the elements in the word list alphabetically. Working with dataframes is harden, hence\n",
    "\twe are curbing this issue by modifying the list rather.'''\n",
    "\tabstract_word_list.sort()\n",
    "\t'''If the word currently being looped in the abstract list matches the trend word being investigated for, the year it appears\n",
    "\tis appended to the permanent word sorter list'''\n",
    "\tfor element in abstract_word_list:\n",
    "\t\tif(element==trend_keywords[0]):\n",
    "\t\t\tpermanent_word_sorter_list.append(abstract_year[:4])\n",
    "\n",
    "\tabstract_word_sorter_end_status_key = \"Added:\"+\" \"+abstract_title+\" \"+\"to the archival list\"\n",
    "\tstatus_logger(status_logger_name, abstract_word_sorter_end_status_key)\n",
    "\n",
    "def abstract_year_list_post_processor(permanent_word_sorter_list, status_logger_name):\n",
    "\t'''Because of this function we have a dictionary containing the frequency of occurrence of terms in specific years'''\n",
    "\tabstract_year_list_post_processor_start_status_key = \"Post processing of permanent word sorter list has commenced\"\n",
    "\tstatus_logger(status_logger_name, abstract_year_list_post_processor_start_status_key)\n",
    "\n",
    "\tstarting_year = min(permanent_word_sorter_list)\n",
    "\tending_year = max(permanent_word_sorter_list)\n",
    "\n",
    "\tabstract_year_dictionary = Counter(permanent_word_sorter_list)\n",
    "\n",
    "\tabstract_year_list_post_processor_end_status_key = \"Post processing of permanent word sorter list has completed\"\n",
    "\tstatus_logger(status_logger_name, abstract_year_list_post_processor_end_status_key)\n",
    "\n",
    "\treturn abstract_year_dictionary, starting_year, ending_year\n",
    "\n",
    "def abstract_year_dictionary_dumper(abstract_word_dictionary, abstracts_log_name, status_logger_name):\n",
    "\t'''This function saves the abstract word dumper to the disc for further inspection.\n",
    "\tThe file is saved as a CSV bucket and then dumped.'''\n",
    "\tpermanent_word_sorter_list_start_status_key = \"Dumping the entire dictionary to the disc\"\n",
    "\tstatus_logger(status_logger_name, permanent_word_sorter_list_start_status_key)\n",
    "\twith open(abstracts_log_name+\"_\"+\"DICTIONARY.csv\", 'w') as dictionary_to_csv:\n",
    "\t\twriter = csv.writer(dictionary_to_csv)\n",
    "\t\tfor key, value in abstract_word_dictionary.items():\n",
    "\t\t\tyear = key\n",
    "\t\t\twriter.writerow([year, value])\n",
    "\t\n",
    "\tpermanent_word_sorter_list_end_status_key = \"Dumped the entire dictionary to the disc\"\n",
    "\tstatus_logger(status_logger_name, permanent_word_sorter_list_end_status_key)\n",
    "\t\t\n",
    "def abstract_page_scraper(abstract_url, abstract_input_tag_id, abstracts_log_name, permanent_word_sorter_list, trend_keywords, site_url_index, status_logger_name):\n",
    "\t'''This function is written to scrape the actual abstract of the specific paper,\n",
    "\t that is being referenced within the list of abstracts'''\n",
    "\tabstract_page_scraper_status_key=\"Abstract ID:\"+\" \"+abstract_input_tag_id\n",
    "\tstatus_logger(status_logger_name, abstract_page_scraper_status_key)\n",
    "\tabstract_page_url = abstract_url+abstract_input_tag_id\n",
    "\tabstract_page = url_reader(abstract_page_url, status_logger_name)\n",
    "\tabstract_soup = page_souper(abstract_page, status_logger_name)\n",
    "\ttitle = title_scraper(abstract_soup, status_logger_name)\n",
    "\tabstract_date = abstract_date_scraper(title, abstract_soup, status_logger_name)\n",
    "\n",
    "\t'''Due to repeated attribute errors with respect to scraping the authors name, these failsafes had to be put in place.'''\n",
    "\ttry:\n",
    "\t\tauthor = author_scraper(abstract_soup, status_logger_name)\n",
    "\texcept AttributeError:\n",
    "\t\tauthor = \"Author not available\"\n",
    "\n",
    "\t'''Due to repeated attribute errors with respect to scraping the abstract, these failsafes had to be put in place.'''\n",
    "\ttry:\n",
    "\t\tabstract = abstract_scraper(abstract_soup)\n",
    "\t\tabstract_word_extractor(abstract, title, abstract_date, permanent_word_sorter_list, trend_keywords, status_logger_name)\n",
    "\texcept AttributeError:\n",
    "\t\tabstract = \"Abstract not available\"\n",
    "\n",
    "\tabstract_database_writer(abstract_page_url, title, author, abstract, abstracts_log_name, abstract_date, status_logger_name)\n",
    "\tanalytical_abstract_database_writer(title, author, abstract, abstracts_log_name, status_logger_name)\n",
    "\n",
    "def abstract_crawler(abstract_url, abstract_id_log_name, abstracts_log_name, permanent_word_sorter_list, trend_keywords, site_url_index, status_logger_name):\n",
    "\tabstract_crawler_temp_index  = site_url_index\n",
    "\t'''This function crawls the page and access each and every abstract'''\n",
    "\tabstract_input_tag_ids = abstract_id_database_reader(abstract_id_log_name, abstract_crawler_temp_index, status_logger_name)\n",
    "\tfor abstract_input_tag_id in abstract_input_tag_ids:\n",
    "\t\ttry:\n",
    "\t\t\tabstract_crawler_accept_status_key=\"Abstract Number:\"+\" \"+str((abstract_input_tag_ids.index(abstract_input_tag_id)+1)+abstract_crawler_temp_index*20)\n",
    "\t\t\tstatus_logger(status_logger_name, abstract_crawler_accept_status_key)\n",
    "\t\t\tabstract_page_scraper(abstract_url, abstract_input_tag_id, abstracts_log_name, permanent_word_sorter_list, trend_keywords, site_url_index, status_logger_name)\n",
    "\t\t\t'''Introduces a 5 second delay between successive pings.'''\n",
    "\t\t\tdelay_function(status_logger_name)\n",
    "\t\texcept TypeError:\n",
    "\t\t\tabstract_crawler_reject_status_key=\"Abstract Number:\"+\" \"+str(abstract_input_tag_ids.index(abstract_input_tag_id)+1)+\" \"+\"could not be processed\"\n",
    "\t\t\tstatus_logger(status_logger_name, abstract_crawler_reject_status_key)\n",
    "\t\t\tpass\n",
    "\n",
    "def analytical_abstract_database_writer(title, author, abstract, abstracts_log_name, status_logger_name):\n",
    "\t'''This function will generate a secondary abstract file that will contain only the abstract.\n",
    "\tThe  abstract file generated will be passed onto the Visualizer and Analyzer function, as opposed to the complete \n",
    "\tabstract log file containing lot of garbage words in addition to the abstract text.'''\n",
    "\tanalytical_abstract_database_writer_start_status_key = \"Writing\"+\" \"+title+\" \"+\"by\"+\" \"+author+\" \"+\"to analytical abstracts file\"\n",
    "\tstatus_logger(status_logger_name, analytical_abstract_database_writer_start_status_key)\n",
    "\n",
    "\tanalytical_abstracts_txt_log = open(abstracts_log_name+'_'+'ANALYTICAL'+'.txt', 'a')\n",
    "\tanalytical_abstracts_txt_log.write(abstract)\n",
    "\tanalytical_abstracts_txt_log.write('\\n'+'\\n')\n",
    "\tanalytical_abstracts_txt_log.close()\n",
    "\n",
    "\tanalytical_abstract_database_writer_stop_status_key = \"Written\"+\" \"+title+\" \"+\"to disc\"\n",
    "\tstatus_logger(status_logger_name, analytical_abstract_database_writer_stop_status_key)\n",
    "\n",
    "def abstract_database_writer(abstract_page_url, title, author, abstract, abstracts_log_name, abstract_date, status_logger_name):\n",
    "\t'''This function makes text files to contain the abstracts for future reference.\n",
    "\tIt holds: 1) Title, 2) Author(s), 3) Abstract'''\n",
    "\tabstract_database_writer_start_status_key = \"Writing\"+\" \"+title+\" \"+\"by\"+\" \"+author+\" \"+\"to disc\"\n",
    "\tstatus_logger(status_logger_name, abstract_database_writer_start_status_key)\n",
    "\tabstracts_csv_log = open(abstracts_log_name+'.csv', 'a')\n",
    "\tabstracts_txt_log = open(abstracts_log_name+'.txt', 'a')\n",
    "\tabstracts_txt_log.write(\"Title:\"+\" \"+title)\n",
    "\tabstracts_txt_log.write('\\n')\n",
    "\tabstracts_txt_log.write(\"Author:\"+\" \"+author)\n",
    "\tabstracts_txt_log.write('\\n')\n",
    "\tabstracts_txt_log.write(\"Date:\"+\" \"+abstract_date)\n",
    "\tabstracts_txt_log.write('\\n')\n",
    "\tabstracts_txt_log.write(\"URL:\"+\" \"+abstract_page_url)\n",
    "\tabstracts_txt_log.write('\\n')\n",
    "\tabstracts_txt_log.write(\"Abstract:\"+\" \"+abstract)\n",
    "\tabstracts_csv_log.write(abstract)\n",
    "\tabstracts_csv_log.write('\\n')\n",
    "\tabstracts_txt_log.write('\\n'+'\\n')\n",
    "\tabstracts_txt_log.close()\n",
    "\tabstracts_csv_log.close()\n",
    "\tabstract_database_writer_stop_status_key = \"Written\"+\" \"+title+\" \"+\"to disc\"\n",
    "\tstatus_logger(status_logger_name, abstract_database_writer_stop_status_key)\n",
    "\n",
    "def abstract_id_database_reader(abstract_id_log_name, site_url_index, status_logger_name):\n",
    "\t'''This function has been explicitly written to access\n",
    "\tthe abstracts database that the given prgram generates.'''\n",
    "\tabstract_id_reader_temp_index = site_url_index\n",
    "\tabstract_id_database_reader_start_status_key = \"Extracting Abstract IDs from disc\"\n",
    "\tstatus_logger(status_logger_name, abstract_id_database_reader_start_status_key)\n",
    "\n",
    "\tlines_in_abstract_id_database=[line.rstrip('\\n') for line in open(abstract_id_log_name+str(abstract_id_reader_temp_index+1)+'.txt')]\n",
    "\n",
    "\tabstract_id_database_reader_stop_status_key = \"Extracted Abstract IDs from disc\"\n",
    "\tstatus_logger(status_logger_name, abstract_id_database_reader_stop_status_key)\n",
    "\n",
    "\treturn lines_in_abstract_id_database\n",
    "\n",
    "def abstract_id_database_writer(abstract_id_log_name, abstract_input_tag_id, site_url_index):\n",
    "\t'''This function writes the abtract ids to a .txt file for easy access and documentation.'''\n",
    "\tabstract_id_writer_temp_index  = site_url_index\n",
    "\tabstract_id_log = open((abstract_id_log_name+str(abstract_id_writer_temp_index+1)+'.txt'), 'a')\n",
    "\tabstract_id_log.write(abstract_input_tag_id)\n",
    "\tabstract_id_log.write('\\n')\n",
    "\tabstract_id_log.close()\n",
    "\n",
    "def abstract_date_scraper(title, abstract_soup, status_logger_name):\n",
    "\t'''This function scrapes the date associated with each of the abstracts.\n",
    "\tThis function will play a crucial role in the functionality that we are trying to build into our project.'''\n",
    "\tdate_scraper_entry_status_key = \"Scraping date of the abstract titled:\"+\" \"+title\n",
    "\tstatus_logger(status_logger_name, date_scraper_entry_status_key)\n",
    "\ttry:\n",
    "\t\tabstract_date = abstract_soup.find('time').get('datetime')\n",
    "\t\tdate_scraper_exit_status_key = title+\" \"+\"was published on\"+\" \"+abstract_date\n",
    "\texcept AttributeError:\n",
    "\t\tabstract_date = \"Date for abstract titled:\"+\" \"+title+\" \"+\"was not available\"\n",
    "\t\tdate_scraper_exit_status_key = abstract_date\n",
    "\t\tpass\n",
    "\t\n",
    "\tstatus_logger(status_logger_name, date_scraper_exit_status_key)\n",
    "\treturn abstract_date\n",
    "\n",
    "def abstract_scraper(abstract_soup):\n",
    "\t'''This function scrapes the abstract from the soup and returns to the page scraper'''\n",
    "\ttry:\n",
    "\t\tabstract = str(abstract_soup.find('p', {'id':'Par1'}).text.encode('utf-8'))[1:]\n",
    "\texcept AttributeError:\n",
    "\t\tabstract = str(abstract_soup.find('p', {'class':'Para'}).text.encode('utf-8'))[1:]\n",
    "\treturn abstract\n",
    "\n",
    "def author_scraper(abstract_soup, status_logger_name):\n",
    "\t'''This function scrapes the author of the text, for easy navigation and search'''\n",
    "\tauthor_scraper_start_status_key = \"Scraping the author name\"\n",
    "\tstatus_logger(status_logger_name, author_scraper_start_status_key)\n",
    "\tauthor = str(abstract_soup.find('span', {'class':'authors__name'}).text.encode('utf-8'))[1:]\n",
    "\tauthor_scraper_end_status_key = \"Scraped the author name\"\n",
    "\tstatus_logger(status_logger_name, author_scraper_end_status_key)\n",
    "\treturn author\n",
    "\n",
    "def title_scraper(abstract_soup, status_logger_name):\n",
    "\t'''This function scrapes the title of the text'''\n",
    "\ttitle_scraper_start_status_key = \"Scraping the title of the abstract\"\n",
    "\tstatus_logger(status_logger_name, title_scraper_start_status_key)\n",
    "\ttry:\n",
    "\t\ttitle = str(abstract_soup.find('h1',{'class':'ArticleTitle'}).text.encode('utf-8'))[1:]\n",
    "\texcept AttributeError:\n",
    "\t\ttitle = str(abstract_soup.find('h1',{'class':'ChapterTitle'}).text.encode('utf-8'))[1:]\n",
    "\ttitle_scraper_end_status_key = \"Scraped the title of the abstract\"\n",
    "\tstatus_logger(status_logger_name, title_scraper_end_status_key)\n",
    "\treturn title\n",
    "\n",
    "def abstract_id_scraper(abstract_id_log_name, page_soup, site_url_index, status_logger_name):\n",
    "\t'''This function helps in obtaining the PII number of the abstract.\n",
    "\tThis number is then coupled with the dynamic URL and provides'''\n",
    "\n",
    "\tabstract_id_scraper_start_status_key=\"Scraping IDs\"\n",
    "\tstatus_logger(status_logger_name, abstract_id_scraper_start_status_key)\n",
    "\t''''This statement collects all the input tags that have the abstract ids in them'''\n",
    "\tabstract_input_tags = page_soup.findAll('a', {'class':'title'})\n",
    "\tfor abstract_input_tag in abstract_input_tags:\n",
    "\t\tabstract_input_tag_id=abstract_input_tag.get('href')\n",
    "\t\tabstract_id_database_writer(abstract_id_log_name, abstract_input_tag_id, site_url_index)\n",
    "\n",
    "\tabstract_id_scraper_stop_status_key=\"Scraped IDs\"\n",
    "\tstatus_logger(status_logger_name, abstract_id_scraper_stop_status_key)\n",
    "\n",
    "def word_sorter_list_generator(status_logger_name):\n",
    "\tword_sorter_list_generator_start_status_key = \"Generating the permanent archival list\"\n",
    "\tstatus_logger(status_logger_name, word_sorter_list_generator_start_status_key)\n",
    "\t'''This function generates the list that hold the Words and corresponding Years of the\n",
    "\tabstract data words before the actual recursion of scrapping data from the website begins.'''\n",
    "\tword_sorter_list = []\n",
    "\n",
    "\tword_sorter_list_generator_exit_status_key = \"Generated the permanent archival list\"\n",
    "\tstatus_logger(status_logger_name, word_sorter_list_generator_exit_status_key)\n",
    "\treturn word_sorter_list\n",
    "\n",
    "def delay_function(status_logger_name):\n",
    "\t'''Since the Springer servers are contstantly shutting down the remote connection, we introduce\n",
    "\tthis function in the processor function in order to reduce the number of pings it delivers to the remote.'''\n",
    "\tdelay_function_start_status_key = \"Delaying remote server ping: 15 seconds\"\n",
    "\tstatus_logger(status_logger_name, delay_function_start_status_key)\n",
    "\n",
    "\tdelay_variable = np.random.randint(0, 10)\n",
    "\t'''Sleep parameter causes the code to be be delayed by 1 second'''\n",
    "\ttime.sleep(delay_variable)\n",
    "\n",
    "\tdelay_function_end_status_key = \"Delayed remote server ping: 15 seconds\"\n",
    "\tstatus_logger(status_logger_name, delay_function_end_status_key)\n",
    "\n",
    "def processor(abstract_url, urls_to_scrape, abstract_id_log_name, abstracts_log_name, status_logger_name, trend_keywords, keywords_to_search):\n",
    "\t''''Multiple page-cycling function to scrape multiple result pages returned from Springer.\n",
    "\tprint(len(urls_to_scrape))'''\n",
    "\t\n",
    "\t'''This list will hold all the words mentioned in all the abstracts. It will be later passed on to the\n",
    "\tvisualizer code to generate the trends histogram.'''\n",
    "\tpermanent_word_sorter_list = word_sorter_list_generator(status_logger_name)\n",
    "\n",
    "\tfor site_url_index in range(0, len(urls_to_scrape)):\n",
    "\n",
    "\t\tif(site_url_index==0):\n",
    "\t\t\tresults_determiner(urls_to_scrape[site_url_index], status_logger_name)\n",
    "\t\t'''Collects the web-page from the url for souping'''\n",
    "\t\tpage_to_soup = url_reader(urls_to_scrape[site_url_index], status_logger_name)\n",
    "\t\t'''Souping the page for collection of data and tags'''\n",
    "\t\tpage_soup = page_souper(page_to_soup, status_logger_name)\n",
    "\t\t'''Scrapping the page to extract all the abstract IDs'''\n",
    "\t\tabstract_id_scraper(abstract_id_log_name, page_soup, site_url_index, status_logger_name)\n",
    "\t\t'''Actually obtaining the abstracts after combining ID with the abstract_url'''\n",
    "\t\tabstract_crawler(abstract_url, abstract_id_log_name, abstracts_log_name, permanent_word_sorter_list, trend_keywords, site_url_index, status_logger_name)\n",
    "\n",
    "\t'''This line of code processes and generates a dictionary from the abstract data'''\n",
    "\t\n",
    "\tabstract_year_dictionary, starting_year, ending_year = abstract_year_list_post_processor(permanent_word_sorter_list, status_logger_name)\n",
    "\n",
    "\treturn abstract_year_dictionary, starting_year, ending_year\n",
    "\n",
    "def scraper_main(abstract_id_log_name, abstracts_log_name, start_url, abstract_url, query_string, trend_keywords, keywords_to_search, status_logger_name):\n",
    "\t''''This function contains all the functions and contains this entire script here, so that it can be imported later to the main function'''\n",
    "\n",
    "\t'''Provides the links for the URLs to be scraped by the scraper'''\n",
    "\turls_to_scrape = url_generator(start_url, query_string, status_logger_name)\n",
    "\t'''Calling the processor() function here'''\n",
    "\tabstract_year_dictionary, starting_year, ending_year = processor(abstract_url, urls_to_scrape, abstract_id_log_name, abstracts_log_name, status_logger_name, trend_keywords, keywords_to_search)\n",
    "\t'''This function dumps the entire dictionary onto the disc for further analysis and inference.'''\n",
    "\tabstract_year_dictionary_dumper(abstract_year_dictionary, abstracts_log_name, status_logger_name)\n",
    "\t'''Returning the abstract word dictionary here'''\n",
    "\treturn abstract_year_dictionary, starting_year, ending_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the ```scraper_main()``` function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_main(abstract_id_log_name, abstracts_log_name, start_url, abstract_url, query_string, trend_keywords, keywords_to_search, status_logger_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
