{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script can be used as a standalone program to process and prepare topic modelling of the ```Scraper.py``` datasets generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 <u>Code:</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import the libraries required to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyResearchInsights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell restarts the kernel so that the installation of the packages can take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import ```dill``` to dump the database before sending it over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries that we installed from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Natural Language toolkit. Here we download the commonly used English stopwords'''\n",
    "import nltk; nltk.download('stopwords')\n",
    "'''Standard set of functions for reading and appending files'''\n",
    "import re\n",
    "'''Pandas and numpy is a dependency used by other portions of the code.'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "'''Think this stands for pretty print. Prints out stuff to the terminal in a prettier way'''\n",
    "from pprint import pprint\n",
    "\n",
    "'''Contains the language model that has to be developed.'''\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "'''Industrial level toolkit for NLP'''\n",
    "import spacy\n",
    "\n",
    "'''Importing OS here to build the directories to hold the status logger logs'''\n",
    "import os\n",
    "\n",
    "'''Importing dill here to dump the database for later retreival'''\n",
    "import dill\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "'''Importing datetime here to log the time when specfic functions are triggered'''\n",
    "from datetime import datetime\n",
    "\n",
    "'''Make pretty visualizations'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''Library to log any errors. Came across this in the tutorial.'''\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'com', 'https', 'url', 'link', 'abstract', 'author', 'chapter', 'springer', 'title', \"the\", \"of\", \"and\", \"in\", \"to\", \"a\", \"is\", \"for\", \"from\", \"with\", \"that\", \"by\", \"are\", \"on\", \"was\", \"as\", \"were\", \"url:\", \"abstract:\", \"abstract\",  \"author:\", \"title:\", \"at\", \"be\", \"an\", \"have\", \"this\", \"which\", \"study\", \"been\", \"not\", \"has\", \"its\", \"also\", \"these\", \"this\", \"can\", \"a\", 'it', 'their', \"e.g.\", \"those\", \"had\", \"but\", \"while\", \"will\", \"when\", \"only\", \"author\", \"title\", \"there\", \"our\", \"did\", \"as\", \"if\", \"they\", \"such\", \"than\", \"no\", \"-\", \"could\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the abstracts log name that will be processed by the rest of the script. The status logger function of the main script has been imported here, and the name of the ```.txt``` file used to log the information has to be added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_log_name = \"Abstract_Database_2020-09-15_11_53\"\n",
    "session_folder_name = abstracts_log_name.split('/')[-1]\n",
    "os.makedirs(session_folder_name)\n",
    "status_logger_name = session_folder_name+\"/\"+\"Status_Logger\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is responsible for generating a ```.txt``` file for analysis once the code has run. This code has been copied directly from the ```common_functions.py``` script of the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_logger(status_logger_name, status_key):\n",
    "\t'''Status logger to print and log details throught the running the program.\n",
    "\tDeclaring current_hour, current_minute & current_second.'''\n",
    "\tcurrent_hour = str(datetime.now().time().hour)\n",
    "\tcurrent_minute = str(datetime.now().time().minute)\n",
    "\tcurrent_second = str(datetime.now().time().second)\n",
    "\t'''Logging the complete_status_key and printing the complete_status_key'''\n",
    "\tcomplete_status_key = \"[INFO]\"+current_hour+\":\"+current_minute+\":\"+current_second+\" \"+status_key\n",
    "\tprint(complete_status_key)\n",
    "\tstatus_log = open(status_logger_name+'.txt', 'a')\n",
    "\tstatus_log.write(complete_status_key+\"\\n\")\n",
    "\tstatus_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we read the ```.txt``` file generated from the journal scrapping, that has to be profiled using topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(abstracts_log_name, status_logger_name):\n",
    "\tdata_reader_start_status_key = abstracts_log_name+\".txt is being ported to dataframe\"\n",
    "\tstatus_logger(status_logger_name, data_reader_start_status_key)\n",
    "\ttextual_dataframe = pd.read_csv(abstracts_log_name+'.txt', delimiter=\"\\t\")\n",
    "\tdata_reader_end_status_key = abstracts_log_name+\".txt has been ported to dataframe\"\t\n",
    "\tstatus_logger(status_logger_name, data_reader_end_status_key)\n",
    "\treturn textual_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the abstracts and preparing a list of abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_data_trimmer(textual_dataframe, status_logger_name):\n",
    "\ttextual_data_trimmer_start_status_key = \"Trimming data and preparing list of words\"\n",
    "\tstatus_logger(status_logger_name, textual_data_trimmer_start_status_key)\n",
    "\n",
    "\t'''This function converts the textual data into a list and removes special characters, virtue of email correspondence'''\n",
    "\ttextual_data = textual_dataframe.values.tolist()\n",
    "\tprint(textual_data)\n",
    "\n",
    "\ttextual_data_trimmer_end_status_key = \"Trimmed data and prepared list of words\"\n",
    "\tstatus_logger(status_logger_name, textual_data_trimmer_end_status_key)\n",
    "\treturn textual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the words here and also removing punctuations from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(textual_data, status_logger_name):\n",
    "\tsent_to_words_start_status_key = \"Tokenizing words\"\n",
    "\tstatus_logger(status_logger_name, sent_to_words_start_status_key)\n",
    "\t'''This function tokenizes each sentence into individual words; also called tokens'''\n",
    "\tfor sentence in textual_data:\n",
    "\t\tyield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\ttextual_data = list(sent_to_words(textual_data))\n",
    "\tsent_to_words_end_status_key = \"Tokenized words\"\n",
    "\tstatus_logger(status_logger_name, sent_to_words_end_status_key)\t\n",
    "\treturn textual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating ***```bigrams```*** from the ```textual_data``` that we have prepared from the abstracts. **Bigrams** are words that occur together, multiple times in a given body of text. For example, *torrential* and *rain* occur together more often with each other, as opposed to other adjective-noun pairs. Hence the two would form a bigram here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_generator(textual_data, status_logger_name):\n",
    "\tbigram_generator_start_status_key = \"Generating word bigrams\"\n",
    "\tstatus_logger(status_logger_name, bigram_generator_start_status_key)\t\n",
    "\t'''Takes the textual data and prepares the bigram, two collectively high frequency words'''\n",
    "\tbigram = gensim.models.Phrases(textual_data, min_count=5, threshold=100)\n",
    "\tbigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\tbigram_generator_end_status_key = \"Generated word bigrams\"\n",
    "\tstatus_logger(status_logger_name, bigram_generator_end_status_key)\t\n",
    "\treturn bigram_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the ```bigram_mod``` model to generate bigrams from our ```corpus```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(textual_data, status_logger_name):\n",
    "\t'''Generates multiple bigrams of word pairs in phrases that commonly occuring with each other over the corpus'''\n",
    "\tmake_bigrams_start_status_key = \"Generating bigrams\"\n",
    "\tstatus_logger(status_logger_name, make_bigrams_start_status_key)\n",
    "\n",
    "\tbigram_mod = bigram_generator(textual_data, status_logger_name)\n",
    "\treturn [bigram_mod[doc] for doc in textual_data]\n",
    "\n",
    "\tmake_bigrams_end_status_key = \"Generated bigrams\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the stopwords, as specified in a list at the start of the program. There's a marked improvement in the topics and keywords produced after expanding the cache of the words to be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(textual_data, status_logger_name):\n",
    "\t'''This function removes the standard set of stopwords from the corpus of abstract words'''\n",
    "\tremove_stopwords_start_status_key = \"Removing stopwords\"\n",
    "\tstatus_logger(status_logger_name, remove_stopwords_start_status_key)\n",
    "\treturn [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in textual_data]\n",
    "\tremove_stopwords_end_status_key = \"Removed stopwords\"\n",
    "\tstatus_logger(status_logger_name, remove_stopwords_end_status_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the ```pyLDAvis``` visualization from the ```lda_model``` generated and trained on our ```corpus``` & ```id2word``` dictionary. The time taken here is proportional to the size of the ```corpus```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizer_generator(lda_model, corpus, id2word, logs_folder_name, status_logger_name):\n",
    "\t'''This code generates the .html file with generates the visualization of the data prepared.'''\n",
    "\tvisualizer_generator_start_status_key = \"Preparing the topic modeling visualization\"\n",
    "\tstatus_logger(status_logger_name, visualizer_generator_start_status_key)\n",
    "    \n",
    "\ttextual_data_visualization = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "\tpyLDAvis.save_html(textual_data_visualization, session_folder_name+'/'+\"Data_Visualization_Topic_Modelling.html\")\n",
    "\n",
    "\tvisualizer_generator_end_status_key = \"Prepared the topic modeling visualization\"+\" \"+\"Data_Visualization_Topic_Modelling.html\"\n",
    "\tstatus_logger(status_logger_name, visualizer_generator_end_status_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization involves truncating a term to its root term. For example *running -> run* is an example of the ```lemmatization``` process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(status_logger_name, textual_data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "\tlemmatization_start_status_key = \"Beginning lemmatization\"\n",
    "\tstatus_logger(status_logger_name, lemmatization_start_status_key)\n",
    "\t\"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "\ttexts_out = []\n",
    "\tnlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\tfor sent in textual_data:\n",
    "\t\tdoc = nlp(\" \".join(sent))\n",
    "\t\ttexts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\tlemmatization_end_status_key = \"Ending lemmatization\"\n",
    "\tstatus_logger(status_logger_name, lemmatization_end_status_key)\n",
    "\treturn texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the ```nlp_engine_main()``` function where all the previously defined functions come together and generate the ```pyLDAvis``` visualization for topic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_engine_main_start_status_key = \"Initiating the NLP Engine\"\n",
    "status_logger(status_logger_name, nlp_engine_main_start_status_key)\n",
    "\n",
    "'''Extracts the data from the .txt file and puts them into a Pandas dataframe buckets'''\n",
    "textual_dataframe = data_reader(abstracts_log_name, status_logger_name)\n",
    "'''Rids the symbols and special characters from the textual_data'''\n",
    "textual_data = textual_data_trimmer(textual_dataframe, status_logger_name)\n",
    "'''Removes stopwords that were earlier downloaded from the textual_data'''\n",
    "textual_data_no_stops = remove_stopwords(textual_data, status_logger_name)\n",
    "'''Prepares bigrams'''\n",
    "textual_data_words_bigrams = make_bigrams(textual_data_no_stops, status_logger_name)\n",
    "'''Loads the English model from spaCy'''\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "textual_data_lemmatized = lemmatization(status_logger_name, textual_data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "id2word = corpora.Dictionary(textual_data_lemmatized)\n",
    "\n",
    "texts = textual_data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "'''Builds the actual LDA model that will be used for the visualization and inference'''\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = 12, random_state = 100, update_every = 1, chunksize = 100, passes = 10, alpha = 'symmetric', per_word_topics = True)\n",
    "\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "perplexity_score = lda_model.log_perplexity(corpus)\n",
    "\n",
    "perplexity_status_key = \"Issued perplexity:\"+\" \"+str(perplexity_score)\n",
    "\n",
    "status_logger(status_logger_name, perplexity_status_key)\n",
    "\n",
    "nlp_engine_main_end_status_key = \"Idling the NLP Engine\"\n",
    "status_logger(status_logger_name, nlp_engine_main_end_status_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presenting the topics of the trained ```LDA``` model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring the ```mallet``` path here. Part of the GitHub package that you've cloned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the ```LDA mallet``` model here, to generate the representative documents, run the coherence tests and issue perplexity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=12, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Just checking out the topics of the ```LDA mallet``` model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presenting the \"Coherence\" of the model here. Coherence is a proxy for how well the model has been built and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=textual_data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presenting the coherence here for a bunch of trained models. Will be eventually selecting the ```optimal_model``` as the ```lda_model```, to get the representative documents with corresponding ```num_topics```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing coherence from the previously generated ```mallet``` models, to find the model with the best coherence, and thereby the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=textual_data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the computed coherence scores here for the models that we generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring the ```optimal_model``` as the previously generated ```ldamallet``` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = ldamallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presenting the topics of the ```ldamallet``` model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics = optimal_model.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```num_words``` dictates the number of keywords to be presented with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(optimal_model.print_topics(num_words=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the dominant topic in each sentence. We use this meta-data as a part of our framework to understand under which areas of research do the publications fall in, and also to label each of the ```pyLDAvis``` topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=textual_data):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the ```dataframes``` here which contains the dominant topics for each abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=textual_data)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the ```dataframe``` to disc for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(session_folder_name+'/Master_Topic_Per_Centence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find the most representative document for each topic, and contain it in a ```dataframe``` and save it as a ```.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "sent_topics_sorteddf_mallet.head()\n",
    "sent_topics_sorteddf_mallet.to_csv(session_folder_name+'/Master_Topics_Contribution.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we find the number of documents for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we find the percentage of documents falling under each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presenting the dominant topics in each sentence here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving it to a ```.csv``` bucket here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topics.to_csv(session_folder_name+'/Master_Topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the visualization here from the ```lda_model```. We use this to label the broad areas of research, in syn which the representative documents generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling the visualizations in the notebook here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the visualization to the disk, as an ```.html``` file in the sessions folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, session_folder_name+'/Master.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presenting the ```pyLDAvis``` visualization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping the entire databse for further analysis. The entire ```session_folder_name``` has to be sent over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session(session_folder_name+'/Comparison_Results.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
